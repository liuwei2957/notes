### zookeeper

#### 分布式协调服务

1. 节点选主
2. 配置管理
3. 粗粒度分布式锁
4. 主备高可用切换
5. 服务注册与发现

#### 适用场合

1. 粗粒度分布式锁，分布式选主，主备高可用切换
2. 不需要高tps支持的场景

### 注册中心

#### 数据一致性

1. 不一致影响节点流量不均衡
2. 对数据一致性要求不高，最终一致性即可

#### 分区容忍性

1. 多机房下注册中心自身为了保证脑裂下的数据一致性而放弃了可用性，导致了同机房的服务之间出现了无法调用，实际生产环境不可取
2. 注册中心不能因为自身的任何原因破坏服务之间本身的连通性

#### 服务规模

1. 服务规模超过一定数量，zk将不堪重负
2. zk主节点只有一个，导致写请求单点，不能水平扩展

##### 解决方法

1. 按照业务功能垂直拆分zk集群
2. 破坏了服务连通性
3. 业务整合连通不可预知

#### 持久化和事务日志

1. 不需要存储，调用方并不关心服务的历史地址列表，过去的健康状态
2. 2pc写数据慢

#### 探活机制

1. 服务的健康检测常利用zk的session活性track机制以及临时节点机制
2. 存在问题：服务假死（死循环）
3. 理想方案：废除心跳检测，服务的健康与否开放给服务提供方自己定义

#### 容灾考虑

1. 服务调用链路需要弱依赖注册中心，必须仅在服务发布，机器上下线，服务扩容缩容时才依赖
2. 客户端应该缓存数据

#### 注册中心设计

##### 注册中心服务注册流程

1. 业务方启动
2. 收集服务信息（spring boot容器）
3. 发送信息（注册中心客户端）
4. 注册节点上报信息（注册中心）

##### 注册中心下发指令流程

1. 业务方提交指令
2. 组装报文（服务管理平台）
3. 请求执行管控指令（服务管理平台）
4. 下发指令（注册中心）
5. 执行命令（业务逻辑层）

##### 服务注册设计

1. rpc/web服务启动时，通过ControllerCenterPlugin与控制中心服务建立并维护长连接，并自动上报服务节点以及节点信息

##### 服务发现设计

1. 主动发现：通过控制中心客户端（ControllerCenterClient），主动查询全部节点信息
2. ControllerCenterPlugin→服务管理平台→ControllerCenterClient→ControllerCenterService
3. 被动发现：服务节点变更时，控制中心服务端主动向ControllerCenterPlugin推送节点变更类型，及变更信息

##### 服务指令设计

1. 服务节点指令：业务方通过服务管理平台或控制中心客户端（需鉴权）可发起屏蔽节点请求
2. 服务质量指令：降级指令（节点/服务/方法），熔断，熔断恢复，限流（节点/服务/方法）
3. 服务配置指令：服务相关配置（超时/重试）

##### 节点注册设计

###### 基于tcp（推荐）

1. 业务节点嵌入控制中心ControllerCenterPlugin，基于Netty通过tcp协议与控制中心服务建立长连接，上报服务节点信息与拓展信息，由服务端进行格式化及存储到mysql
2. 数据持久化到mysql/tidb
3. 注册数据控制中心服务读写mysql
4. 缓存加速必要性

###### 基于etcd（不推荐）

业务节点嵌入etcd客户端，基于grpc通过http2协议与etcd集群建立长连接，获取服务节点信息与拓展信息，由客户端完成etcd节点的格式化，创建及存储

##### 客户端与控制中心通信

ControllerCenterClient端通信协议：http协议

##### 控制中心集群节点发现

###### 基于gossip协议（推荐）

1. 基于gossip算法维护集群节点的自动发现，转移，心跳
2. gossip协议成熟度高（p2p网络通信/redis cluster/consul）

缺点

1. 消息延迟，消息冗余

###### 基于etcd（不推荐）

1. 客户端适用etcd，控制中心集群的应用层，无需考虑节点发现逻辑
2. raft算法

##### 指令同步

###### 基于tcp（推荐）

1. 控制中心服务集群任意节点，接收到服务指令
2. 基于netty通过tcp长连接，在集群内进行服务指令同步

###### 基于消息队列

1. 控制中心服务集群任意节点接收到服务指令
2. 基于消息队列发布服务指令
3. 控制中心服务集群全部节点从消息队列接收服务指令

###### 基于etcd

1. 服务中心客户端基于etcd watch服务指令目录，当客户端发现服务指令目录非空时，顺次执行服务指令，并在内存内部标记偏移量
2. 重启时会将服务指令清空

##### 指令下发

1. 方案一（推荐）：控制中心服务端集群，接收到服务指令，通过之前维护的ControllerCenterPlugin连接，进行指令推送，因指令具备时效性，可接收一定程度的损耗
2. 方案二：服务中心客户端基于etcd watch服务指令目录，当客户端发现服务指令目录非空时，获取最上端的指令进行执行，执行后删除执行后的指令

##### 高可用设计

###### ControllerCenterPlugin

1. ControllerCenterPlugin与ControllerCenterService维护长连接，当与ControllerCenterService连接断开时，重新连接，请求其它节点
2. 服务节点信息本地缓存
3. 重连失败时报警

###### ControllerCenterService

1. 服务间集群通过gossip协议检测成员存活状况
2. 当成员死亡时，ControllerCenterPlugin与ControllerCenterService连接断开，重新连接，同时进行上报

##### 重点细节

1. ControllerCenterPlugin获取ControllerCenterService的ip（dns域名方式）
2. ControllerCenterService新加和重启数据如何同步（注册信息，指令信息）
3. ControllerCenterService gossip探活目的（死掉节点剔除集群）
4. 什么时候剔除死掉的注册服务节点（长连接断开到连接断开事件，补偿，读时写，服务管理平台读取显示时，配置文件更新时，思路是根据ip的update time新旧为依据，10分钟没更新即踢掉）

### 配置中心

#### 定义

1. 应用服务配置统一存储和管理的可视化系统
2. 配置是独立于程序的只读变量
3. 伴随应用的整个生命周期
4. 配置可以有多种加载方式
5. 配置需要治理（权限控制，不同环境，集群配置管理）

#### apollo总体架构

1. ConfigService提供配置的读取，推送等功能，服务对象是apollo客户端
2. Admin Service提供配置的修改，发布等功能，服务对象是Apollo Portal（管理界面）
3. Config Service和Admin Service都是多实例，无状态部署，所以需要将自己注册到Eureka中并保持心跳
4. 在Eureka之上架了一层Meta Server用于封装Eureka的服务发现接口
5. Client通过域名访问Meta Server获取Config Service服务列表（ip+port），而后直接通过ip+port访问服务，同时在Client侧会做load balance，错误重试
6. Portal通过域名访问Meta Server获取Admin Service服务列表（ip+port），而后直接通过ip+port访问服务，同时在Portal侧会做load balance，错误重试
7. Config Service，Eureka和Meta Server三个逻辑角色部署在同一个jvm进程中

#### apollo客户端设计

1. 客户端和服务端保持一个长连接，从而能第一时间获得配置更新的推送
2. 客户端还会定时从apollo配置中心服务端拉取应用的最新配置
   1. 这是一个fallback机制，为了防止推送机制失败导致配置不更新
   2. 客户端定时拉取，并上报本地版本
3. 客户端从apollo配置中心服务端获取到应用的最新配置后，会保存在内存中
4. 客户端会把从服务端获取到的配置在本地文件系统缓存一份
5. 应用程序从apollo客户端获取最新的配置，订阅配置更新通知

#### apollo配置更新推送实现

1. 长连接实际上是基于http long polling实现
2. 客户端发起一个请求到服务端，服务端会保持这个请求60s

#### 配置中心设计

1. 管理员在Portal操作配置中心→mysql
2. 配置项缓存到redis，变更通知zk
3. 客户端从zk收到配置项变更事件
4. 客户端从redis拉取配置
5. 客户端把配置项缓存在本地