### 合理利用 CPU 的高性能

1. CPU 增加了缓存，以均衡与内存的速度差异
2. 操作系统增加了进程、线程，以分时复用 CPU，进而均衡 CPU 与 I/O 设备的速度差异
3. 编译程序优化指令执行次序，使得缓存能够得到更加合理地利用

### 并发程序问题根源

1. 缓存导致的可见性问题，一个线程对共享变量的修改，另外一个线程能够立刻看到，称为可见性
2. 线程切换带来的原子性问题，在一个时间片内，如果一个进程进行一个 IO 操作，例如读个文件，这个时候该进程可以把自己标为“休眠状态”并出让 CPU 的使用权，待文件读进内存，操作系统会把这个休眠的进程唤醒，唤醒后的进程就有机会重新获得 CPU 的使用权了，这里的进程在等待 IO 时之所以会释放 CPU 使用权，是为了让 CPU 在这段等待时间里可以做别的事情，这样一来 CPU 的使用率就上来了，操作系统做任务切换，可以发生在任何一条CPU 指令执行完，而不是高级语言里的一条语句，我们把一个或者多个操作在 CPU 执行的过程中不被中断的特性称为原子性，CPU能保证的原子操作是 CPU 指令级别的，而不是高级语言的操作符
3. 编译优化带来的有序性问题，有序性指的是程序按照代码的先后顺序执行。编译器为了优化性能，有时候会改变程序中语句的先后顺序(new 操作符执行步骤：分配一块内存 M，在内存 M 上初始化对象，然后 M 的地址赋值给引用变量)

### Happens-Before原则

前面一个的操作结果对后续操作是可见的

1.  程序的顺序性规则，指在一个线程中，按照程序顺序，前面的操作 Happens-Before 于后续的任意操作
2. volatile 变量规则，指对一个 volatile 变量的写操作， Happens-Before 于后续对这个 volatile 变量的读操作
3. 传递性，如果 A Happens-Before B，且 B Happens-Before C，那么 A Happens-Before C
4. 管程中锁的规则，指对一个锁的解锁 Happens-Before 于后续对这个锁的加锁
5.  线程start() 规则，指主线程A启动子线程B后，子线程B能够看到主线程在启动子线程B前的操作
6. 线程join() 规则，指主线程 A 等待子线程 B 完成（主线程 A 通过调用子线程 B的 join() 方法实现），当子线程 B 完成后（主线程 A 中 join() 方法返回），主线程能够看到子线程的操作。当然所谓的“看到”，指的是对共享变量的操作

### 原子性

一个或者多个操作在 CPU 执行的过程中不被中断的特性，称为“原子性”，原子性问题的源头是线程切换，而操作系统做线程切换是依赖 CPU 中断的，所以禁止 CPU 发生中断就能够禁止线程切换

#### 锁

我们把一段需要互斥执行的代码称为临界区

##### 死锁

一组互相竞争资源的线程因互相等待，导致“永久”阻塞的现象

###### 死锁条件

1. 互斥，共享资源 X 和 Y 只能被一个线程占用
2. 占有且等待，线程 T1 已经取得共享资源 X，在等待共享资源 Y 的时候，不释放共享资源 X
3. 不可抢占，其他线程不能强行抢占线程 T1 占有的资源
4. 循环等待，线程 T1 等待线程 T2 占有的资源，线程 T2 等待线程 T1 占有的资源，就是循环等待

###### 预防死锁

1. 对于“占有且等待”这个条件，我们可以一次性申请所有的资源，这样就不存在等待了
2. 对于“不可抢占”这个条件，占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源(synchronized做不到，Lock可以)
3. 对于“循环等待”这个条件，可以靠按序申请资源来预防。需要对资源进行排序，然后按序申请资源，所谓按序申请，是指资源是有线性顺序的，申请的时候可以先申请资源序号小的，再申请资源序号大的

解决死锁问题最好的办法还是规避死锁

##### synchronized

1. 当修饰静态方法的时候，锁定的是当前类的 Class 对象
2. 当修饰非静态方法的时候，锁定的是当前实例对象 this

### 锁与资源的关系

1. 受保护资源和锁之间的关联关系是 N:1 的关系，可以用一把锁来保护多个资源，但是不能用多把锁来保护一个资源，**一个合理的受保护资源与锁之间的关联关系应该是 N:1**”。只有 共享一把锁才能起到互斥的作用，加锁本质就是在锁对象的对象头中写入当前线程id
2. 用不同的锁对受保护资源进行精细化管理，能够提升性能，叫细粒度锁
3. 锁要能覆盖所有受保护资源，如果资源之间没有关系，很好处理，每个资源一把锁就可以了。如果资源之间有关联关系，就要选择一个粒度更大的锁，这个锁应该能够覆盖所有相关的资源，除此之外，还要梳理出有哪些访问路径，所有的访问路径都要设置合适的锁
4. 关联关系如果用更具体、更专业的语言来描述的话， 其实是一种“原子性”特征，我们提到的原子性，主要是面向 CPU 指令的
5. “原子性”的本质是什么?其实不是不可分割，不可分割只是外在表现，其本质是多个资源 间有一致性的要求，操作的中间状态对外不可见，解决原子性问题，是要保证中间状态对外不 可见
6. 使用细粒度锁是有代价的，这个代价就是可能会导致死锁
7. **锁，应是私有的、不可变的、不可 重用的**，Integer 和 String 类型的对象不适合做锁，因为Integer和 String 类型的对象在 JVM 里面是可能被重用的

// 普通对象锁
 private final Object lock = new Object();
 // 静态对象锁
 private static final Object lock = new Object();

### 等待通知机制

线程首先获取互斥锁，当线程要求的条件不满足时，释放互斥锁，进入等待状态;当要求的条件满足时，通知等待的线程，重新获取互斥锁，Java 语言内置的synchronized 配合 wait()、notify()、notifyAll() 这三个方法就能轻松实现

1. 当调用 wait() 方法后，当前线程就会被阻塞，并且进入到右边的等待队列中，**这个等待队列也是互斥锁的等待队列**。 线程在进入等待队列的同时，**会释放持有的互斥锁**，线程释放锁后，其他线程就有机会获得 锁，并进入临界区了
2. 当条件满足时调 用 notify()，会通知等待队列(**互斥锁的等待队列**)中的线程，告诉它**条件曾经满足过**，**notify() 只能保证在通知时间点，条件是满足的**。而被通 知线程的**执行时间点和通知的时间点**基本上不会重合，所以当线程执行的时候，很可能条件 已经不满足了(保不齐有其他线程插队，要循环判断)
3. 被通知的线程要想重新执行，仍然需要获取到互斥锁 (因为曾经获取的锁在调用 wait() 时已经释放了)
4. **notify() 是会随机地通知等待队列中的一个线程，而 notifyAll() 会通知等 待队列中的所有线程**，使用 notify() 也很有风险，它的风险在于可能导致某些线程永远不会被通知到

### 安全性

**存在共享数据并且该数据会发生变化，通俗地讲就是有多个线程会同时读写同一 数据**

1. 当多个线程同时访问同一数据，并且至少有一个线程会写这个数据的时候，如果我们不采取防护措施，那么就会导致数据竞争

#### 竞态条件

**竞态条件，指的是程序的执行结果依赖线程执行的顺序**，也就是依赖于某个状态变量

#### 活跃性

指的是某个操作无法执行下去。我们常见的“死锁”就是一种典型的活跃 性问题，当然**除了死锁外，还有两种情况，分别是“活锁”和“饥饿”**

##### 活锁

**有时线程虽然没有发生阻塞，但仍然会存在执行不下去的情况，这就是所谓的“活锁”**

1. 解决“**活锁**”的方案很简单，谦让时，尝试等待一个随机的时间就可以了

##### 饥饿

**所谓“饥饿”指的是线程因无法访问所需资源而无法执行下去 的情况**

不患寡，而患不均”，如果线程优先级“不均”，在 CPU 繁忙的情况下，优先 级低的线程得到执行的机会很小，就可能发生线程“饥饿”;持有锁的线程，如果执行的时 间过长，也可能导致“饥饿”问题

1. 一是保证资源充足，二是公平地分配资源， 三就是避免持有锁的线程长时间执行
2. 使用公平锁分配资源，所谓公平锁，是一种先来后到的方案，线程的等待是有顺序的，排在等待队列前面的线程会优先获得资源

#### 性能问题

1. 最好的方案自然就是使用无锁的算法和数据结构，如线程本地存储 (Thread Local Storage, TLS)、写入时复 制 (Copy-on-write)、乐观锁等;Java 并发包里面的原子类也是一种无锁的数据结构; Disruptor 则是一个无锁的内存队列
2. 减少锁持有的时间，如使用细粒度的锁，一个典型 的例子就是 Java 并发包里的ConcurrentHashMap，它使用了所谓分段锁的技术，还可以使用读写锁，也就是读是无锁的，只有写的时候才会互 斥

##### 性能度量指标

1. 吞吐量:指的是单位时间内能处理的请求数量。吞吐量越高，说明性能越好
2. 延迟:指的是从发出请求到收到响应的时间。延迟越小，说明性能越好
3. 并发量:指的是能同时处理的请求数量，一般来说随着并发量的增加、延迟也会增加

### 管程

**管程和信号量是 等价的，所谓等价指的是用管程能够实现信号量，也能用信号量实现管程**,管程对应的英文是 Monitor，也叫监视器，**管程，指的是管理共享变量以及对共享变量的操作过程，让他们支持并发**，管程解决互斥问题的思路很简单，就是将共享变量及其对共享变量的操作统一封装起来，管程里还引入了条件变量的概念，而且**每个条件变量都对应有一个等待队列**

1. **Java SDK 并发包通过 Lock 和 Condition 两个接口来实现管程，其 中 Lock 用于解决互斥问题，Condition 用于解决同步问题**

#### **MESA** **模型**

假设有个线程 T1 执行出队操作，不过需要注意的是执行出队操作，有个前提条件，就是队 列不能是空的，而队列不空这个前提条件就是管程里的条件变量。 如果线程 T1 进入管程 后恰好发现队列是空的，那怎么办呢?等待啊，去哪里等呢?就去条件变量对应的等待队列 里面等。此时线程 T1 就去“队列不空”这个条件变量的等待队列中等待，再假设之后另外一个线程 T2 执行入队操作，入队操作执行成功之后，“队列不空”这个条 件对于线程 T1 来说已经满足了，此时线程 T2 要通知 T1，告诉它需要的条件已经满足了。 当线程 T1 得到通知后，会从等待队列里面出来，但是出来之后不是马上执行，而是重新进 入到入口等待队列里面

```
public class BlockedQueue<T> {
    final Lock lock = new ReentrantLock();
    // 条件变量:队列不满
    final Condition notFull = lock.newCondition();
    // 条件变量:队列不空
    final Condition notEmpty = lock.newCondition();

    //入队
    void enq(T x) {
        lock.lock();
        try {
            while (队列已满) {
                // 等待队列不满
                notFull.await();
            }
            // 入队后, 通知可出队
            notEmpty.signal();
        } finally {
            lock.unlock();
        }
    }

    //出队
    void deq() {
        lock.lock();
        try {
            while (队列已空) {
                // 等待队列不空
                notEmpty.await();
            }
            // 出队后，通知可入队
            notFull.signal();
        } finally {
            lock.unlock();
        }
    }
}
```

1. 对于 MESA 管程来说，有一个编程范式，就是需要在一个while 循环里面调用 wait()。**这个是 MESA 管程特有的**
2. MESA 管程里面，T2 通知完 T1 后，T2 还是会接着执行，T1 并不立即执行，仅仅是从 条件变量的等待队列进到入口等待队列里面。这样做的好处是 notify() 不用放到代码的 最后，T2 也没有多余的阻塞唤醒操作。但是也有个副作用，就是当 T1 再次执行的时 候，可能曾经满足的条件，现在已经不满足了，所以需要以循环方式检验条件变量

#### Lock

Lock&Condition 是管程的一种实现

#### Condition

### 信号量模型

1. 一个计数器，一个等待队列，三个方法
2. init()：设置计数器的初始值
3. down()：计数器的值减 1；如果此时计数器的值小于 0，则当前线程将被阻塞，否则当前线程可以继续执行
4. up()：计数器的值加 1；如果此时计数器的值小于或者等于 0，则唤醒等待队列中的一个线程，并将其从等待队列中移除

### 线程

#### 生命周期

1. 初始状态，指的是线程已经被创建，但是还不允许分配 CPU 执行。这个状态属于编程语 言特有的，不过这里所谓的被创建，仅仅是在编程语言层面被创建，而在操作系统层 面，真正的线程还没有创建
2. **可运行状态**，指的是线程可以分配 CPU 执行。在这种状态下，真正的操作系统线程已经 被成功创建了，所以可以分配 CPU 执行

3. 当有空闲的 CPU 时，操作系统会将其分配给一个处于可运行状态的线程，被分配到 CPU 的线程的状态就转换成了**运行状态**
4. 运行状态的线程如果调用一个阻塞的 API(例如以阻塞方式读文件)或者等待某个事件 (例如条件变量)，那么线程的状态就会转换到**休眠状态**，同时释放 CPU 使用权，休眠 状态的线程永远没有机会获得 CPU 使用权。当等待的事件出现了，线程就会从休眠状态 转换到可运行状态
5. 线程执行完或者出现异常就会进入**终止状态**，终止状态的线程不会切换到其他任何状 态，进入终止状态也就意味着线程的生命周期结束了

#### **Java** 线程生命周期

1. NEW(初始化状态)
2. RUNNABLE(可运行 / 运行状态)
3. BLOCKED(阻塞状态)
4. WAITING(无时限等待)
5. TIMED_WAITING(有时限等待)
6. TERMINATED(终止状态)

Java 线程中的 BLOCKED、WAITING、TIMED_WAITING 是一种状态，即前面我们提到的休眠状态。也 就是说**只要 Java 线程处于这三种状态之一，那么这个线程就永远没有 CPU 的使用权**

##### 线程状态转换

###### **RUNNABLE** **与** **BLOCKED** **的状态转换**

1. 线程等待 synchronized 的隐式锁，等待的 线程就会从 RUNNABLE 转换到 BLOCKED 状态。而当等待的线程获得 synchronized 隐 式锁时，就又会从 BLOCKED 转换到 RUNNABLE 状态
2. 线程调用阻塞式 API 时，在操作系统层面，线程是会转换到休眠状态的，但是在 JVM 层面，Java 线程的状态不会发生变化，也就是说 Java 线程的状态会依然保持 RUNNABLE 状态。**JVM 层面并不关心操作系统调度相关的状态**，因为在 JVM 看来，等待 CPU 使用权(操作系统层面此时处于可执行状态)与等待 I/O(操作系统层面此时处于休 眠状态)没有区别，都是在等待某个资源，所以都归入了 RUNNABLE 状态

###### **RUNNABLE** **与** **WAITING** **的状态转换**

1. 获得 synchronized 隐式锁的线程，调用无参数的 Object.wait() 方法
2. 调用无参数的 Thread.join() 方法。其中的 join() 是一种线程同步方法
3. 调用 LockSupport.park() 方法，Java 并发包中的锁，都是基于它实现的

###### **RUNNABLE** **与** **TIMED_WAITING** **的状态转换**

1. 调用**带超时参数**的 Thread.sleep(long millis) 方法
2. 获得 synchronized 隐式锁的线程，调用**带超时参数**的 Object.wait(long timeout) 方法
3. 调用**带超时参数**的 Thread.join(long millis) 方法
4. 调用**带超时参数**的 LockSupport.parkNanos(Object blocker, long deadline) 方法
5. 调用**带超时参数**的 LockSupport.parkUntil(long deadline) 方法

###### **从** **NEW** **到** **RUNNABLE** **状态**

1. Java 刚创建出来的 Thread 对象就是 NEW 状态，NEW 状态的线程，不会被操作系统调度，因此不会执行。Java 线程要执行，就必须转换到 RUNNABLE 状态。从 NEW 状态转换到 RUNNABLE 状态很简单，只要调用线程对象的 start() 方法就可以了

###### **从** **RUNNABLE** **到** **TERMINATED** **状态**

线程执行完 run() 方法后，会自动转换到 TERMINATED 状态，当然如果执行 run() 方法的 时候异常抛出，也会导致线程终止，强制中断 run() 方法的执行，可以调用 interrupt() 方法

##### **stop() 和 interrupt() 方法**区别

1. stop() 方法会真的杀死线程，不给线程喘息的机会，如果线程持有 ReentrantLock 锁，被 stop() 的线程并不会自动调用 ReentrantLock 的 unlock() 去释放锁，那其他线程就再也没 机会获得 ReentrantLock 锁，隐式锁会释放掉
2. interrupt() 方法仅仅是通知线程，线程有机会执行一些后 续操作，同时也可以无视这个通知，其他线程调用线程 A 的 interrupt() 方法会使线程 A 返回到 RUNNABLE 状态，同时线程 A 的代码会触发 InterruptedException 异常
3. 在触发 InterruptedException 异常的同时，JVM 会同时把线程的中断标志位清除，在捕获异常之后要重新设置中断标志位

###### 如何收到通知

1. 异常，抛出异常后，中断标示会自动清除 掉，当线程 A 处于 WAITING、TIMED_WAITING 状态，或当线程 A 处于 RUNNABLE 状态时，并且阻塞在io上
2. 主动检测，线程处于 RUNNABLE 状态，并且没有阻塞在某个 I/O 操作上，这时就得依赖线程 A 主动检测中断状态了。如果其他线程调用线程 A 的 interrupt() 方法，那么线程 A 可以通过 isInterrupted() 方法，检测是不是自己被中断了

#### 多线程优点

提升程序性能，**在并发编程领域，提升性能本质上就是提升硬件的利用率，再具体点来说，就是提升 I/O 的利用率和 CPU 的利用率**，**如果 CPU 和 I/O 设备的利用率都很低，那么可以尝 试通过增加线程来提高吞吐量**

#### 线程数量

1. 对于 CPU 密集型计算，多线程本质上是提升多核 CPU 的利用率，每个核一个线程，再多创建线程也只是增加线程切 换的成本，所以，**对于 CPU 密集型的计算场景，理论上“线程的数量 =CPU 核数”就是 最合适的**。不过在工程上，**线程的数量一般会设置为“CPU 核数 +1”**，这样的话，当线程因为偶尔的内存页失效或其他原因导致阻塞时，这个额外的线程可以顶上，从而保证 CPU 的利用率
2. 对于 I/O 密集型的计算，最佳的线程数是与程序中 CPU 计算和 I/O 操作的耗时比相关的，对于单核cpu，最佳线程数 =1 +(I/O 耗时 / CPU 耗时)，令 R=I/O 耗时 / CPU 耗时，可以这样理解:当线程 A 执行 IO 操作时，另 外 R 个线程正好执行完各自的 CPU 计算。这样 CPU 的利用率就达到了 100%，对于多核cpu，最佳线程数 =CPU 核数 * [ 1 +(I/O 耗时 / CPU 耗时)]
3. 对于I/0密集型的计算，粗略计算，最佳线程数应该为:2 * CPU 的核数 + 1
4. 随着线程数的增加，吞吐量会增加， 延迟也会缓慢增加;但是当线程数增加到一定程度，吞吐量就会开始下降，延迟会迅速增加

#### 方法调用

每个方法在调用栈里都有自己的独立空间，称为**栈帧**， 每个栈帧里都有对应方法需要的参数和返回地址。当调用方法时，会创建新的栈帧，并压入 调用栈;当方法返回时，对应的栈帧就会被自动弹出。也就是说，**栈帧和方法是同生共死 的**，**局部变量也放到 了调用栈里**，局部变量是和方法同生共死的，一个变量如果想跨 越方法的边界，就必须创建在堆里

1. 方法的调用，是先计算参数，然后将参数压入调用栈之后才会执行方法体

#### 调用栈与线程

**每个线程都有自己独立的调用栈**

#### 两阶段终止模式

1. 发送终止指令
2. 响应终止指令
3. Java 线程进入终止状态的前提是线程进入 RUNNABLE 状态，而实际上线程也可能处在休眠状态，也就是说，我们要想终止一个线程，首先要把线程的状态从休眠状态转换到 RUNNABLE 状态，需要调用interrupt()方法
4. 优雅的方式是让 Java 线程自己执行完 run() 方法，所以一般我们采用的方法是设置一个标志位，然后线程会在合适的时机检查这个标志位，如果发现符合终止条件，则自动退出 run() 方法
5. 也可以用一个"毒丸"对象

### 面向对象并发编程

#### 封装共享变量

1. **将共享变量作为对象属性封装在内部，对所有公共方法制定并发访问策略**，要避免“逸出”，所谓“逸出”简单讲就是共享变量逃逸到对象的外面
2. **对于这些不会发生变化的共享变量，建议用 final 关键字来修饰**

#### 识别共享变量间的约束条件

1. 当代码里出现 if 语句的时候，就应该立刻意识到可能存在竞态条件

#### 制定并发访问策略

1. 避免共享，避免共享的技术主要是利于线程本地存储以及为每个任务分配独立的线程
2. 不变模式:这个在 Java 领域应用的很少，但在其他领域却有着广泛的应用，例如 Actor模式、CSP 模式以及函数式编程的基础都是不变模式
3. 管程及其他同步工具:Java 领域万能的解决方案是管程，但是对于很多特定场景，使用Java 并发包提供的读写锁、并发容器等同步工具会更好

### Juc

#### synchronized

synchronized 没有办法解决死锁**破坏不可抢占条件**。原因是 synchronized 申请资源的时候，如果申请不 到，线程直接进入阻塞状态了，而线程进入阻塞状态，啥都干不了，也释放不了线程已经占有的资源

#### 破坏不可抢占条件

1. 能够响应中断，synchronized 的问题是，持有锁 A 后，如果尝试获取锁 B 失败，那么 线程就进入阻塞状态，一旦发生死锁，就没有任何机会来唤醒阻塞的线程。但如果阻塞 状态的线程能够响应中断信号，也就是说当我们给阻塞的线程发送中断信号的时候，能 够唤醒它，那它就有机会释放曾经持有的锁 A。这样就破坏了不可抢占条件了
2. 支持超时，如果线程在一段时间之内没有获取到锁，不是进入阻塞状态，而是返回一个 错误，那这个线程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件
3. 非阻塞地获取锁，如果尝试获取锁失败，并不进入阻塞状态，而是直接返回，那这个线 程也有机会释放曾经持有的锁。这样也能破坏不可抢占条件

#### Lock保证可见性

1. 顺序性规则
2. volatile变量规则
3. 传递性规则

#### 可重入锁

指的是线程可以重复获取同一把锁

#### 公平锁与非公平锁

如果一个线程没有获得锁，就会进入等待队列，当有线程释放锁的时候，就需要从等待队列中唤醒一个等待的线程。如果是公平锁，唤醒的策略就是谁等待的时间长，就唤醒谁，很公平；如果是非公平锁，则不提供这个公平保证，有可能等待时间短的线程反而先被唤醒

#### 用锁的最佳实践

1. 永远只在更新对象的成员变量时加锁
2. 永远只在访问可变的成员变量时加锁
3. 永远不在调用其他对象的方法时加锁（也许“其他”方法里面有线程sleep() 的调用，也可能会有奇慢无比的 I/O 操作，这些都会严重影响性能。更可怕的是，“其他”类的方法可能也会加锁，然后双重加锁就可能导致死锁）

#### 读写锁

1. 允许多个线程同时读共享变量
2. 只允许一个线程写共享变量
3. 如果一个写线程正在执行写操作，此时禁止读线程读共享变量

#### StampedLock

1. 写锁
2. 悲观读锁
3. 乐观读，这个操作是无锁的
4. StampedLock 提供的乐观读，是允许一个线程获取写锁的，也就是说不是所有的写操作都被阻塞
5. 其中，写锁、悲观读锁的语义和 ReadWriteLock的写锁、读锁的语义非常类似，允许多个线程同时获取悲观读锁，但是只允许一个线程获取写锁，写锁和悲观读锁是互斥的，不同的是：StampedLock里的写锁和悲观读锁加锁成功之后，都会返回一个stamp；然后解锁的时候，需要传入这个stamp

##### 比较

1. 对于读多写少的场景 StampedLock 性能很好，简单的应用场景基本上可以替代ReadWriteLock
2. StampedLock 不支持重入
3. StampedLock 的悲观读锁、写锁都不支持条件变量

#### CountDownLatch

CountDownLatch 主要用来解决一个线程等待多个线程的场景，CountDownLatch 的计数器是不能循环利用的，也就是说一旦计数器减到 0，再有线程调用 await()，该线程会直接通过

#### CyclicBarrier

CyclicBarrier 是一组线程之间互相等待，CyclicBarrier 的计数器是可以循环利用的，而且具备自动重置的功能，一旦计数器减到 0 会自动重置到你设置的初始值。除此之外，CyclicBarrier 还可以设置回调函数

#### 并发容器

##### list

1. CopyOnWriteArrayList，写的时候会将共享变量新复制一份出来，这样做的好处是读操作完全无锁
2. 仅适用于写操作非常少的场景，而且能够容忍读写的短暂不一致
3. 迭代器是只读的，不支持增删改。因为迭代器遍历的仅仅是一个快照，而对快照进行增删改是没有意义的

##### map

1. ConcurrentHashMap，ConcurrentSkipListMap
2. ConcurrentHashMap的key是无序的，而ConcurrentSkipListMap的key 是有序的， key和value都不能为空
3. SkipList插入、删除、查询操作平均的时间复杂度是 O(log n)，理论上和并发线程数没有关系

##### set

1. CopyOnWriteArraySet，ConcurrentSkipListSet

##### queue

Java 并发包里阻塞队列都用 Blocking 关键字标识，单端队列使用Queue标识，双端队列使用 Deque 标识

###### 单端阻塞队列

1. ArrayBlockingQueue，LinkedBlockingQueue，SynchronousQueue，LinkedTransferQueue，PriorityBlockingQueue，DelayQueue
2. 内部一般会持有一个队列，这个队列可以是数组（其实现是ArrayBlockingQueue）也可以是链表（其实现是 LinkedBlockingQueue）；甚至还可以不持有队列（其实现是 SynchronousQueue），此时生产者线程的入队操作必须等待消费者线程的出队操作。而 LinkedTransferQueue 融合 LinkedBlockingQueue 和SynchronousQueue 的功能，性能比 LinkedBlockingQueue 更好；PriorityBlockingQueue 支持按照优先级出队；DelayQueue 支持延时出队

###### 双端阻塞队列

1. LinkedBlockingDeque

###### 单端非阻塞队列

1. ConcurrentLinkedQueue

###### 双端非阻塞队列

1. ConcurrentLinkedDeque

使用队列时，需要格外注意队列是否支持有界（所谓有界指的是内部的队列是否有容量限制）。一般都不建议使用无界的队列，因为数据量大了之后很容易导致

OOM。只有 ArrayBlockingQueue 和LinkedBlockingQueue 是支持有界的

#### 原子类

##### 原理

CPU 为了解决并发问题，提供了 CAS 指令（CAS，全称是 Compare And Swap，即“比较并交换”）。CAS 指令包含 3 个参数：共享变量的内存地址 A、用于比较的值 B 和共享变量的新值 C；并且只有当内存中地址 A 处的值等于 B 时，才能将内存中地址 A 处的值更新为新值 C。作为一条 CPU 指令，CAS 指令本身是能够保证原子性的

##### 无锁

无锁方案相对于互斥锁方案，优点非常多，首先性能好，其次是基本不会出现死锁问题（但可能出现饥饿和活锁问题，因为自旋会反复重试）

#### 线程池

线程是一个重量级的对象，应该避免频繁创建和销毁，线程池是一种生产者-消费者模型

##### 参数

1. corePoolSize：表示线程池保有的最小线程数
2. maximumPoolSize：表示线程池创建的最大线程数
3. keepAliveTime & unit：如果一个线程空闲了keepAliveTime & unit这么久，而且线程池的线程数大于corePoolSize ，那么这个空闲的线程就要被回收了
4. workQueue：工作队列
5. threadFactory：自定义如何创建线程
6. handler：自定义任务的拒绝策略，如果线程池中所有的线程都在忙碌，并且工作队列也满了（前提是工作队列是有界队列），那么此时提交任务，线程池就会拒绝接收

###### 拒绝策略

1. CallerRunsPolicy：提交任务的线程自己去执行该任务
2. AbortPolicy：默认的拒绝策略，会 throws RejectedExecutionException
3. DiscardPolicy：直接丢弃任务，没有任何异常抛出
4. DiscardOldestPolicy：丢弃最老的任务，其实就是把最早进入工作队列的任务丢弃，然后把新任务加入到工作队列

##### 注意点

1. 不建议使用 Executors 的最重要的原因是：Executors 提供的很多方法默认使用的都是无界的 LinkedBlockingQueue，高负载情境下，无界队列很容易导致 OOM
2. 如果任务在执行的过程中出现运行时异常，会导致执行任务的线程终止，也获取不到任何通知，线程池提供了很多用于异常处理的方法，但是最稳妥和简单的方案还是捕获所有异常并按需处理

#### future

##### 方法

1. 取消任务的方法 cancel()
2. 判断任务是否已取消的方法 isCancelled()
3. 判断任务是否已结束的方法 isDone()
4. 2 个获得任务执行结果的 get() 和 get(timeout, unit)

##### CompletableFuture

1. 默认情况下 CompletableFuture 会使用公共的 ForkJoinPool 线程池，这个线程池默认创建的线程数是 CPU 的核数
2. 一旦有任务执行一些很慢的I/O 操作，就会导致线程池中所有线程都阻塞在 I/O 操作上，从而造成线程饥饿，进而影响整个系统的性能。强烈建议根据不同的业务类型创建不同的线程池，以避免互相干扰

##### CompletionService

1. 当需要批量提交异步任务的时候建议使用 CompletionService。CompletionService 将线程池 Executor 和阻塞队列 BlockingQueue 的功能融合在了一起，能够让批量异步任务的管理更简单

##### 总结

对于简单的并行任务，你可以通过“线程池 +Future”的方案来解决；如果任务之间有聚合关系，无论是 AND 聚合还是 OR 聚合，都可以通过 CompletableFuture 来解决；而批量的并行任务，则可以通过 CompletionService 来解决

#### fork-join

Fork/Join 是一个并行计算的框架，主要就是用来支持分治任务模型的，这个计算框架里的Fork 对应的是分治任务模型里的任务分解，Join 对应的是结果合并

1. Fork/Join 计算框架主要包含两部分，一部分是分治任务的线程池 ForkJoinPool，另一部分是分治任务ForkJoinTask

#### 限流算法

1. 令牌桶，令牌桶算法是定时向令牌桶发送令牌，请求能够从令牌桶中拿到令牌，然后才能通过限流器
2. 漏桶，漏桶算法里，请求就像水一样注入漏桶，漏桶会按照一定的速率自动将水漏掉，只有漏桶里还能注入水的时候，请求才能通过限流器
