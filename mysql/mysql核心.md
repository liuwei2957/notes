## innodb底层

### 数据页

InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候，并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小默认是16KB

### change buffer

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内存，然后执行change buffer中与这个页有关的操作，change buffer也会被写入到磁盘上，唯一索引的更新就不能使用change buffer，因为必须要将数据页读入内存才能判断记录是否唯一

1. change buffer因为减少了随机磁盘访问，所以对更新性能的提升是会很明显的
2. 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好

#### 与redo log比较

1. redo log主要节省的是随机写磁盘的IO消耗（转成顺序写），而change buffer主要节省的则是随机读磁盘的IO消耗

### 优化器的逻辑

1. 优化器选择索引的目的，是找到一个最优的执行方案，并用最小的代价去执行语句。在数据库里面，扫描行数是影响执行代价的因素之一。扫描的行数越少，意味着访问磁盘数据的次数越少，消耗的CPU资源越少
2. 扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表、是否排序等因素进行综合判断
3. 实索引统计只是一个输入，对于一个具体的语句来说，优化器还要判断，执行这个语句本身要扫描多少行
4. 使用普通索引需要把回表的代价算进去

#### 扫描行数判断

1. MySQL在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数
2. 这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好。而一个索引上不同的值的个数，称之为“基数”（cardinality）。也就是说，这个基数越大，索引的区分度越好
3. MySQL通过采样统计的方式得到索引的基数值

## 日志

### redo log(重做日志)

#### 原因

1. 如果每次更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高，为此MySQL引入了WAL技术，WAL的全称是Write- Ahead Logging，它的关键点就是先写日志，再写磁盘，当有一条记录需要更新的时候，InnoDB引擎会先把记录写到redo log里面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做

#### 特点

1. redo log大小是固定的，可以配置为一组4个文件，每个文件的大小是 1GB，那么redo log总共就可以记录4GB的操作
2. 从头开始写，write pos是当前记录的位置，一边写一边后移，写到最后一个文件末尾就回到0号文件开头，checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件
3. write pos和checkpoint之间的是redo log上还空着的部分，可以用来记录新的操作，如果write pos追上checkpoint，表示满了，这时候不能再执行新的更新，得停下来先清理一些记录，把checkpoint推进一下
4. redo log可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个 能力称为crash-safe

#### 设置

innodb_flush_log_at_trx_commit这个参数设置成1的时候，表示每次事务的redo log都直接持久化到磁盘。这个参数建议设置成1，这样可以保证MySQL异常重启之后数据不丢失

#### 脏页flush

1. InnoDB的redo log写满了。这时候系统会停止所有更新操作，把checkpoint往前推进，redo log留出空间可以继续写
2. 当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘
3. MySQL认为系统“空闲”的时候
4. MySQL正常关闭的情况

#### 刷脏页的控制策略

1. 一个是脏页比例，一个是redo log写盘速度

无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用IO资源并可能影响到了你的更新语句，都可能是造成你从业务端感知到MySQL“抖”了一下的原因，MySQL中的一个机制，可能让你的查询会更慢：在准备刷一个脏页的时候，如果这个数据页旁边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉；而且这个把“邻居”拖下水的逻辑还可以继续蔓延，也就是对于每个邻居数据页，如果跟它相邻的数据页也还是脏页的话，也会被放到一起刷

#### binlog写完,redolog还没commit

1. 如果redo log里面的事务是完整的，也就是已经有了commit标识，则直接提交
2. 如果redo log里面的事务只有完整的prepare，则判断对应的事务binlog是否存在并完整，如果是，则提交事务

### binlog(归档日志)

#### 原因

Server层也有自己的日志，称为binlog(归档日志)，只依靠binlog是没有crash-safe能力的，InnoDB使用另外一套日志系统，也就是redo log来实现crash-safe能力

#### 不同点

1. redolog是InnoDB引擎特有的;binlog是MySQL的Server层实现的，所有引擎都可以使用
2. redolog是物理日志，记录的是“在某个数据页上做了什么修改”;binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”
3. redolog是循环写的，空间固定会用完;binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志

#### 设置

sync_binlog这个参数设置成1的时候，表示每次事务的binlog都持久化到磁盘。这个参数建议设置成1，这样可以保证MySQL异常重启之后binlog不丢失

#### binlog格式

1. statement，记录的是sql语句的原文，可能导致主备不一致
2. row，binlog里面记录了真实删除行的主键id，不会有主备删除不同行的问题，缺点是很占空间
3. mixed，MySQL自己会判断这条SQL语句是否可能引起主备不一致，如果有可能，就用row格式，否则就用statement格式

#### mysql判断binlog是否完整

1. statement格式的binlog，最后会有COMMIT
2. row格式的binlog，最后会有一个XID event
3. 在MySQL 5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确性

#### redo log 和 binlog是怎么关联起来的

它们有一个共同的数据字段，叫XID

1. 如果碰到既有prepare、又有commit的redo log，就直接提交
2. 如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务
3. 处于prepare阶段的 阶redo log加上完整binlog，重启就能恢复，这时候binlog已经写入了，之后就会被从库（或者用这个binlog恢复出来的库）使用，所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性

### 执行器update流程

1. 执行器先找引擎取满足条件的这一行。ID是主键，引擎直接用树搜索找到这一行。如果这一行所在的数据页本来就在内存中，就直接返回给执行器；否则，需要先从磁盘读入内存，然后再返回
2. 执行器拿到引擎给的行数据，更新相应的字段，得到新的一行数据，再调用引擎接口写入这行新数据
3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redo log里面，此时redo log处于prepare状态。然后告知执行器执行完成了，随时可以提交事务
4. 执行器生成这个操作的binlog，并把binlog写入磁盘
5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redo log改成提交（commit）状态，更新完成(redo log的写入是两阶段提交)

#### 两阶段提交

这是为了让两份日志之间的逻辑一致，对于InnoDB引擎来说，如果redo log提交完成了，事务就不能回滚(如果这还允许回滚，就可能覆盖掉别的事务的更新)。而如果redo log直接提交，然后binlog写入的时候失败，InnoDB又回滚不了，数据和binlog日志又不一致了

假设当前ID=2的行，字段c的值是0，要更新成1

1. 先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异常重启。redo log写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行c的值是1。 但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份日志的时候，存起来的binlog里面就没有这条语句。 如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同

2. 先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日志。所以在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是 1，与原库的值不同

   可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致，简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致

## 事务

### 隔离性

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot

#### 隔离级别

1. 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到

2. 读已提交是指，一个事务提交之后，它做的变更才会被其他事务看到

3. 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一 致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的

4. 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行

#### 实现

数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级别下，这个视图是在每个SQL语句开始执行的时候创建的。需要注意的是，“读未提交”隔离级别下直接返回记录上的最新值，没有视图概念;而“串行化”隔离级别下直接用加锁的方式来避免并行访问

##### 可重复读实现

1. 在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值，当没有事务再需要用到这些回滚日志时，也就是系统里没有比这个回滚日志更早的read-view的时候，回滚日志会被删除
2. 可重复读的核心就是一致性读（consistent read）；而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待

### 长事务风险

1. 长事务意味着系统里面会存在很老的事务视图。由于这些事务随时可能访问数据库里面的任何数据，所以这个事务提交之前，数据库里面它可能用到的回滚记录都必须保留，这就会导致大量占用存储空间
2. 长事务还占用锁资源，也可能拖垮整个库

### 视图

InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持RC（Read Committed，读提交）和RR（Repeatable Read，可重复读）隔离级别的实现。它没有物理结构，作用是事务执行期间用来定义“事务能看到什么数据”

#### 实现

InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的，每次事务更新数据的时候，都会生成一个新的数据版本，并且把transaction id赋值给这个数据版本的事务ID，记为rowtrx_id。同时旧的数据版本要保留，并且在新的数据版本中，能够有信息可以直接拿到它

### MVCC

InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活跃”的所有事务ID。“活跃”指的就是，启动了但还没提交，数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位

#### 实现

对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能

1. 小于等于低水位，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的
2. 大于等于高水位，表示这个版本是由将来启动的事务生成的，是肯定不可见的
3. 大于低水位，小于高水位，
   1. 若 row trx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见
   2. 若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以 外，有三种情况:

1. 版本未提交，不可见
2. 版本已提交，但是是在视图创建后提交的，不可见
3. 版本已提交，而且是在视图创建前提交的，可见

更新数据都是先读后写的，而这个读，只能读当前的值，称为“当前读”（current read）

#### 读提交和可重复读区别

1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都共用这个一致性视图
2. 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图

## 索引

索引的出现其实就是为了提高数据查询的效率，就像书的目录一样

### 常见模型

#### 哈希表

哈希表是一种以键-值（key-value）存储数据的结构，根据key就可以找到其对应的值即Value。哈希的思路就是把值放在数组里，用一个哈希函数把key换算成一个确定的位置，然后把value放在数组的这个位置，多个key值经过哈希函数的换算，会出现同一个值的情况。处理这种情况的一种方法是，拉出一个链表，因为不是有序的，所以哈希索引做区间查询的速度很慢，哈希表这种结构适用于只有等值查询的场景

#### 有序数组

有序数组在等值查询和范围查询场景中的性能就都非常优秀，但是在需要更新数据的时候就麻烦了，往中间插入一个记录就必须得挪动后面所有的记录，成本太高，所以有序数组索引只适用于静态存储引擎

#### 二叉搜索树

二叉搜索树的特点是：每个节点的左儿子小于父节点，父节点又小于右儿子，二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上

#### N叉树

为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。就不应该使用二叉树，而是要使用“N叉”树，N叉树就是每个节点有多个儿子，儿子之间的大小保证从左到右递增。这里，“N叉”树中的“N”取决于数据块的大小

### InnoDB 索引模型

1. 在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表，InnoDB使用了B+树索引模型，所以数据都是存储在B+树中的
2. 每一个索引在InnoDB里面对应一棵B+树
3. 根据叶子节点的内容，索引类型分为主键索引和非主键索引，主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引（clustered index），非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引（secondary index）
4. 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小
5. 基于非主键索引的查询需要多扫描一棵索引树

#### 特点

1. 回到主键索引树搜索的过程，我们称为回表
2. 不需要回表称为覆盖索引，覆盖索引可以减少树的搜索次数，显著提升查询性能

#### 索引维护

1. B+树为了维护索引有序性，在插入新值的时候需要做必要的维护
2. 可能需要申请一个新的数据页，然后挪动部分数据过去。这个过程称为页分裂，除了性能外，页分裂操作还影响数据页的利用率
3. 当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并

#### 最左前缀原则

1. 索引项是按照索引定义里面出现的字段顺序排序的
2. 不只是索引的全部定义，只要满足最左前缀，就可以利用索引来加速检索，这个最左前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符

#### 联合索引顺序

评估标准是，索引的复用能力

1. 第一原则是，如果通过调整顺序，可以减少维护一个索引，那么这个顺序往往就是需要优先考虑采用的
2. 其次是索引占用的空间

#### 索引下推

可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数

#### 前缀索引

可以定义字符串的一部分作为索引，使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本。但使用前缀索引用不上覆盖索引对查询性能的优化

##### 前缀索引优化

1. 倒序存储
2. 第二种方式是使用hash字段

两种方式都不支持范围查询

###### 区别

1. 从占用的额外空间来看，倒序存储方式在主键索引上，不会消耗额外的存储空间，而hash字段方法需要增加一个字段
2. 在CPU消耗方面，倒序方式每次写和读的时候，都需要额外调用一次reverse函数，而hash字段的方式需要额外调用一次crc32()函数。如果只从这两个函数的计算复杂度来看的话，reverse函数额外消耗的CPU资源会更小些
3. 从查询效率上看，使用hash字段方式的查询性能相对更稳定一些。因为crc32算出来的值虽然有冲突的概率，但是概率非常小，可以认为每次查询的平均扫描行数接近1。而倒序存储方式毕竟还是用的前缀索引的方式，也就是说还是会增加扫描行数

### 索引选择异常

1. 对于由于索引统计信息不准确导致的问题，你可以用analyze table来解决
2. 采用force index选择索引
3. 修改语句，引导MySQL选择正确的索引
4. 新建一个更合适的索引或删掉误用的索引

### 普通索引和唯一索引

1. 尽量选择普通索引，这两类索引在查询能力上是没差别的，主要考虑的是对更新性能的影响
2. 如果所有的更新后面，都马上伴随着对这个记录的查询，那么应该关闭change buffer。而在其他情况下，change buffer都能提升更新性能

## 锁

### 分类

#### 全局锁

全局锁就是对整个数据库实例加锁，使用场景是做全库逻辑备份，在备份过程中整个库完全处于只读状态

#### 表级锁

1. 一种是表锁
2. 一种是元数据锁(MDL)，MDL不需要显示使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁
3. MDL会直到事务提交才释放，在做表结构变更的时候，一定要小心不要导致锁住线上查询和更新

#### 行锁

1. MySQL的行锁是在引擎层由各个引擎自己实现的

#### 间隙锁

1. 间隙锁之间都不存在冲突关系
2. next-key lock记为前开后闭区间
3. 了读提交隔离级别下，在语句执行完成后，是只有行锁的。而且语句执行完成后，InnoDB就会把不满足条件的行行锁去掉，不需要等到事务提交

##### 两阶段锁协议

在InnoDB 事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放，这个就是两阶段锁协议，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放

#### 死锁

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致这几个线程都进入无限等待的状态，称为死锁

##### 两种策略

1. 一种策略是，直接进入等待，直到超时
2. 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行

##### 解决办法

1. 控制并发度，基本思路就是，对于相同行的更新，在进入引擎之前排队，也可以考虑通过将一行改成逻辑上的多行来减少锁冲突

### 加锁规则

1. 加锁的基本单位是next-key lock，next-key lock是前开后闭区间
2. 查找过程中访问到的对象才会加锁
3. 索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁
4. 索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁
5. 唯一索引上的范围查询会访问到不满足条件的第一个值为止
6. “有行”才会加行锁。如果查询条件没有命中行，那就加next-key lock

## 性能优化

### count(*)的实现方式

1. MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高
2. InnoDB引擎，在执行count(*)的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数

### Innodb为什么不把数字存起来

是因为即使是在同一个时刻的多个查询，由于多版本并发控制（MVCC）的原因，InnoDB表“应该返回多少行”也是不确定的

1. MySQL优化器会找到最小的那棵树来遍历。在保证逻辑正 在 确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一

### 计数方法

#### 用缓存系统保存计数

1. 缓存系统可能会丢失更新,redis异常重启，数据还未持久化
2. Redis异常重启以后，到数据库里面单独执行一次count(*)获取真实的行数，再把这个值写回到Redis里就可以了
3. 数据不一致问题

#### 在数据库保存计数

1. 用事务避免数据不一致

### 不同的count用法

count()是一个聚合函数，对于返回的结果集，一行行地判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值

1. count(*)、count(主键id)和count(1) 都表示返回满足条件的结果集的总行数；而count(字段），则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数
2. 对 count( 主键主 id) 来说 ，InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server 层。server层拿到id后，判断是不可能为空的，就按行累加
3. 对于对 count(1) 来说 ，InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个 数字“1”进去，判断是不可能为空的，按行累加
4. count(*) c 是例外 是 ，并不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是null，按行累加

### join

1. 使用join语句，性能比强行拆成多个单表执行SQL语句的性能要好
2. 如果使用join语句的话，需要让小表做驱动表
3. 在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，在过滤完成之后，计算参与join的各个字段的总数据量，数据量小的那个表，就是 “小表 ”，应该作为驱动表

### order by

#### 全字段排序

1. Extra这个字段中的“Using filesort”表示的就是需要排序，MySQL会给每个线程分配一块内存用于排序，称为sort_buffer
2. 如果要排序的数据量小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序
3. sort_buffer_size越小，需要分成的份数越多，number_of_tmp_files的值就越大

#### rowid排序

1. 是如果查询要返回的字段很多的话，那么sort_buffer里面要放的字段数太多，这样内存里能够同时放下的行数很少，要分成很多个临时文件，排序的性能会很差
2. sort buffer只放id和排序字段

##### 比较

1. 如果MySQL实在是担心排序内存太小，会影响排序效率，才会采用rowid排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据
2. 如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据
3. 文件排序采用归并排序，limit数字比较小的时候采用堆排序

#### sql优化

1. 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃 走树搜索功能，并不是放弃使用这个索引，而是使用全索引扫描
2. 隐式类型转换，会触发函数操作
3. 隐式字符编码转换，会触发函数操作

#### 查询慢

##### 查询长时间不返回

1. 等MDL锁，现在有一个线程正在表上请求或者持有MDL写锁，把select语句堵住了
2. 等flush，关闭表
3. 等行锁

##### 查询慢

#### 自增主键不连续

1. 唯一键冲突
2. 事务回滚，自增主键不能回退
3. 批量新增(insert select，不知道需要申请多少个id，每次申请id数量增加一倍)
