### mysql服务器逻辑架构

1. Server层包括连接器、查询缓存、分析器、优化器、执行器等，涵盖MySQL的大多数核心服务 功能，以及所有的内置函数(如日期、时间、数学和加密函数等)，所有跨存储引擎的功能都在 这一层实现，比如存储过程、触发器、视图等
2. 而存储引擎层负责数据的存储和提取。其架构模式是插件式的，支持InnoDB、MyISAM、 Memory等多个存储引擎，存储引擎api包含几十个底层函数，用于执行诸如，开始一个事务，根据主键提取一行记录等操作，存储引擎不会去解析sql(Innodb会解析外键定义，因为mysql服务器本身没有实现该功能)

### sql执行顺序

#### 查询

1. 建立连接，建立连接的过程通常是比较复杂的，所以建议在使用中要尽量减少建立连接的动作，也就是尽量使用长连接，但是全部使用长连接后，你可能会发现，有些时候MySQL占用内存涨得特别快，这是因为 MySQL在执行过程中临时使用的内存是管理在连接对象里面的。这些资源会在连接断开的时候 才释放。所以如果长连接累积下来，可能导致内存占用太大，被系统强行杀掉（1. 定期断开长连接。使用一段时间，或者程序里面判断执行过一个占用内存的大查询后，断开连接，之后要查询再重连 2. 如果你用的是MySQL5.7或更新版本，可以在每次执行一个比较大的操作后，通过执行 mysql_reset_connection来重新初始化连接资源。这个过程不需要重连和重新做权限验证， 但是会将连接恢复到刚刚创建完时的状态）
2. 查询缓存，之前执行过的语句及其结果可能会以key-value对的形式，被直接缓存在内存中。key是查询的语句，value是 查询的结果（但是大多数情况下建议不要使用查询缓存，为什么呢?因为查询缓存往往弊大于利，查询缓存的失效非常频繁，只要有对一个表的更新，这个表上所有的查询缓存都会被清空。因此很可能你费劲地把结果存起来，还没使用呢，就被一个更新全清空了。对于更新压力大的数据库 来说，查询缓存的命中率会非常低。除非你的业务就是有一张静态表，很长时间才会更新一次。 比如，一个系统配置表，那这张表上的查询才适合使用查询缓存，MySQL 8.0版本直接将查询缓存的整块功能删掉了，也就是说8.0开始彻底没有这个功能了）
3. 分析器，分析器先会做“词法分析”。你输入的是由多个字符串和空格组成的一条SQL语句，MySQL需要识别出里面的字符串分别是什么，代表什么，做完了这些识别以后，就要做“语法分析”。根据词法分析的结果，语法分析器会根据语法规则， 判断你输入的这个SQL语句是否满足MySQL语法
4. 优化器，优化器是在表里面有多个索引的时候，决定使用哪个索引;或者在一个语句有多表关联(join)的时候，决定各个表的连接顺序
5. 执行器，开始执行的时候，要先判断一下你对这个表T有没有执行查询的权限，如果没有，就会返回没有 权限的错误，如果有权限，就打开表继续执行。打开表的时候，执行器就会根据表的引擎定义，去使用这个引擎提供的接口，第一次调用的是“取满足条件的第一行”这个接口，之后循环取“满足条件的下一行”这个接口，直到取到这个表的最后一行，执行器将上述遍历过程中所有满足条件的行组成的记录集作为结果集返回给客户端，你会在数据库的慢查询日志中看到一个rows_examined的字段，表示这个语句执行过程中扫描了多少行。这个值就是在执行器每次调用引擎获取数据行的时候累加的

#### 更新

如果每一次的更新操作都需要写进磁盘，然后磁盘也要找到对应的那条记录，然后再更新，整个过程IO成本、查找成本都很高，为此MySQL引入了WAL技术，WAL的全称是Write- Ahead Logging，它的关键点就是先写日志，再写磁盘，具体来说，当有一条记录需要更新的时候，InnoDB引擎就会先把记录写到redo log里 面，并更新内存，这个时候更新就算完成了。同时，InnoDB引擎会在适当的时候，将这个操作记录更新到磁盘里面，而这个更新往往是在系统比较空闲的时候做，InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是 1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写，write pos是当前记录的位置，一边写一边后移，checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件，有了redo log，InnoDB就可以保证即使数据库发生异常重启，之前提交的记录都不会丢失，这个 能力称为crash-safe

#### 数据删除流程

InnoDB引擎只会把这个记录标记为删除。如果之后要再插入一个记录时，可能会复用这个位置。但是，磁盘文件的大小并不会缩小，如果我们删掉了一个数据页上的所有记录，整个数据页就可以被复用了，数据页的复用跟记录的复用是不同的，记录的复用，只限于符合范围条件的数据，而当整个页从B+树里面摘掉以后，可以复用到任何位置，如果相邻的两个数据页利用率都很小，系统就会把这两个页上的数据合到其中一个页上，另外一 个数据页就被标记为可复用，delete命令其实只是把记录的位置，或者数据页标记为了“可复用”，但磁盘文件 的大小是不会变的，经过大量增删改的表，都是可能是存在空洞的。所以，如果能够把这些空洞去掉，就能达到收缩表空间的目的，而重建表，就可以达到这样的目的，alter语句在启动的时候需要获取MDL写锁，但是这个写锁在真正拷贝数据之前就退化成读锁了，那为什么不干脆直接解锁呢?为了保护自己，禁止其他线程对这个表同时做DDL

##### binlog

redo log是 InnoDB引擎特有的日志，而Server层也有自己的日志，称为binlog(归档日志)，只依靠binlog是没有crash-safe能力的，InnoDB使用另外一套日志系统— — 也就是redo log来实现crash-safe能力

##### 不同点

1. redolog是InnoDB引擎特有的;binlog是MySQL的Server层实现的，所有引擎都可以使用
2. redolog是物理日志，记录的是“在某个数据页上做了什么修改”;binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”
3. redolog是循环写的，空间固定会用完;binlog是可以追加写入的。“追加写”是指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志

##### 执行流程

1. 执行器先找引擎取ID=2这一行。ID是主键，引擎直接用树搜索找到这一行。如果ID=2这一 行所在的数据页本来就在内存中，就直接返回给执行器;否则，需要先从磁盘读入内存，然 后再返回

2. 执行器拿到引擎给的行数据，把这个值加上1，比如原来是N，现在就是N+1，得到新的一行 数据，再调用引擎接口写入这行新数据

3. 引擎将这行新数据更新到内存中，同时将这个更新操作记录到redolog里面，此时redolog处 于prepare状态。然后告知执行器执行完成了，随时可以提交事务

4. 执行器生成这个操作的binlog，并把binlog写入磁盘

5. 执行器调用引擎的提交事务接口，引擎把刚刚写入的redolog改成提交(commit)状态，更新完成

   最后三步看上去有点“绕”，将redo log的写入拆成了两个步骤:prepare和commit，这就是"两阶段提交"

### mysql崩溃恢复

binlog写完，redo log还没commit前发生 crash，如果redolog里面的事务是完整的，也就是已经有了commit标识，则直接提交，如果redolog里面的事务只有完整的prepare，则判断对应的事务binlog是否存在并完整，如果是，则提交事务，否则，回滚事务

#### 为什么必须有“两阶段提交”

这是为了让两份日志之间的逻辑一致，对于InnoDB引擎来说，如果redo log提交完成了，事务就不能回滚(如果这还允许回滚，就可能 覆盖掉别的事务的更新)。而如果redo log直接提交，然后binlog写入的时候失败，InnoDB又回 滚不了，数据和binlog日志又不一致了

假设当前ID=2的行，字段c的值是0，要更新1

1. 先写redo log后写binlog。假设在redo log写完，binlog还没有写完的时候，MySQL进程异 常重启。由于我们前面说过的，redo log写完之后，系统即使崩溃，仍然能够把数据恢复回 来，所以恢复后这一行c的值是1。 但是由于binlog没写完就crash了，这时候binlog里面就没有记录这个语句。因此，之后备份 日志的时候，存起来的binlog里面就没有这条语句。 然后你会发现，如果需要用这个binlog来恢复临时库的话，由于这个语句的binlog丢失，这 个临时库就会少了这一次更新，恢复出来的这一行c的值就是0，与原库的值不同

2. 先写binlog后写redo log。如果在binlog写完之后crash，由于redo log还没写，崩溃恢复以 后这个事务无效，所以这一行c的值是0。但是binlog里面已经记录了“把c从0改成1”这个日 志。所以，在之后用binlog来恢复的时候就多了一个事务出来，恢复出来的这一行c的值就是 1，与原库的值不同

   可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的 状态不一致

#### MySQL怎么判断binlog是完整的

一个事务的binlog是有完整格式的

1. statement格式的binlog，最后会有COMMIT

2. row格式的binlog，最后会有一个XID event

3. 在MySQL 5.6.2版本以后，还引入了binlog-checksum参数，用来验证binlog内容的正确 性。对于binlog日志由于磁盘原因，可能会在日志中间出错的情况，MySQL可以通过校验

   checksum的结果来发现

#### redo log 和 binlog是怎么关联

它们有一个共同的数据字段，叫XID。崩溃恢复的时候，会按顺序扫描redo log

1. 如果碰到既有prepare、又有commit的redo log，就直接提交
2. 如果碰到只有parepare、而没有commit的redo log，就拿着XID去binlog找对应的事务

#### 处于prepare阶段的redo log加上完整binlog，重启就能恢复，MySQL为什么要这么设计

数据与备份的一致性有关。在binlog写完以后MySQL发生崩溃，这时候binlog已经写入了，之后就会被从库(或者用这个 binlog恢复出来的库)使用，所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性

#### 只用redo log，不要binlog

如果只从崩溃恢复的角度来讲是可以的。你可以把binlog关掉，这样就没有两阶段提交 了，但系统依然是crash-safe的

#### redolog

1. redo log用于保证crash-safe能力。innodb_flush_log_at_trx_commit这个参数设置成1的时候， 表示每次事务的redo log都直接持久化到磁盘。这个参数建议设置成1，这样可以保证 MySQL异常重启之后数据不丢失

### 事务的ACID特性

#### 原子性

一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚

#### 一致性

数据库总是从一个一致性的状态转移到另外一个一致性的状态，因为事务最终没有提交，所以事务中所做的修改也不会保存到数据库中

#### 隔离性

通常来说，一个事务所做的修改在最终提交以前，对其他事务是不可见的

#### 持久性

一旦事务提交，则其所做的修改就会永久保存到数据库中，即使系统崩溃，修改的数据也不会丢失(不可能有能做到100%的持久性保证策略)

### 隔离性

1. 读未提交是指，一个事务还没提交时，它做的变更就能被别的事务看到

2. 读提交是指，一个事务提交之后，它做的变更才会被其他事务看到

3. 可重复读是指，一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一 致的。当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的

4. 串行化，顾名思义是对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突 的时候，后访问的事务必须等前一个事务执行完成，才能继续执行

   在实现上，数据库里面会创建一个视图，访问的时候以视图的逻辑结果为准。在“可重复读”隔离 级别下，这个视图是在事务启动时创建的，整个事务存在期间都用这个视图。在“读提交”隔离级 别下，这个视图是在每个SQL语句开始执行的时候创建的。这里需要注意的是，“读未提交”隔离 级别下直接返回记录上的最新值，没有视图概念;而“串行化”隔离级别下直接用加锁的方式来避免并行访问

#### 事务隔离的实现

在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值，当没有事务再需要用到这些回滚日志时，也就是系统里没有比这个回滚日志更早的read-view的时候，回滚日志会被删除

### 索引

InnoDB的数据是按数据页为单位来读写的。也就是说，当需要读一条记录的时候， 并不是将这个记录本身从磁盘读出来，而是以页为单位，将其整体读入内存。在InnoDB中，每个数据页的大小默认是16KB

1. 哈希索引，哈希表这种结构适用于只有等值查询的场景，比如Memcached及其他一些NoSQL引擎
2. 有序数组，有序数组在等值查询和范围查询场景中的性能就都非常优秀，在需要更新数据的时候就麻烦 了，你往中间插入一个记录就必须得挪动后面所有的记录，成本太高，有序数组索引只适用于静态存储引擎
3. 二叉搜索树，二叉树是搜索效率最高的，但是实际上大多数的数据库存储却并不使用二叉树。其原因是，索引不止存在内存中，还要写到磁盘上，为了让一个查询尽量少地读磁盘，就必须让查询过程访问尽量少的数据块。那么，我们就不应该使用二叉树，而是要使用“N叉”树。这里，“N叉”树中的“N”取决于数据块的大小

#### InnoDB 的索引模型

在InnoDB中，表都是根据主键顺序以索引的形式存放的，这种存储方式的表称为索引组织表

1. 主键索引的叶子节点存的是整行数据。在InnoDB里，主键索引也被称为聚簇索引(clustered index)
2. 非主键索引的叶子节点内容是主键的值。在InnoDB里，非主键索引也被称为二级索引 (secondary index)

#### 索引维护

B+树为了维护索引有序性，在插入新值的时候需要做必要的维护，需要申请一个新的 数据页，然后挪动部分数据过去。这个过程称为页分裂。在这种情况下，性能自然会受影响，除了性能外，页分裂操作还影响数据页的利用率。原本放在一个页的数据，现在分到两个页中， 整体空间利用率降低大约50%，当然有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合 并的过程，可以认为是分裂过程的逆过程

1. 自增主键的插入数据模式，正符合了我们前面提到的递增插入的场景。每次插入一条新记录，都是追加操作，都不涉及到挪动其他记录，也不会触发叶子节点的分裂
2. 而有业务逻辑的字段做主键，则往往不容易保证有序插入，这样写数据成本相对较高
3. 主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小
4. 由于InnoDB是索引组织表，一般情况下建议创建一个自增主键，这样非主键索引占用的 空间最小
5. 只有一个索引，该索引必须是唯一索引适合用业务字段直接做主键
6. 由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用 的性能优化手段

#### 最左前缀原则

1. B+树这种索引结构，可以利用索引的“最左前缀”，来定位记录，这个最左 前缀可以是联合索引的最左N个字段，也可以是字符串索引的最左M个字符
2. 索引项是按照索引定义里面出现的字段顺序排序的

#### 联合索引

评估标准是，索引的复用能力以及空间占用

1. 第一原则是，如果通过调整顺序，可 以少维护一个索引，那么这个顺序往往就是需要优先考虑采用的

#### 索引下推

1. 在MySQL 5.6之前，只能从找到满足条件的索引开始一个个回表。到主键索引上找出数据行，再对比字段值
2. 而MySQL 5.6 引入的索引下推优化(index condition pushdown)， 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数

#### 查询过程

先是通过B+树从树根开始，按层搜索到叶子节点的这个数据页，然后可以认为数据页内部通过二分法来定位记录，因为引擎是按页读写的，所以说，当找到满足条件的记录的时候，它所在的数据页就都在内存里了。 那么，对于普通索引来说，要多做的那一次“查找和判断下一条记录”的操作，就只需要一次指针寻找和一次计算，如果这个记录刚好是这个数据页的最后一个记录，那么要取下一个记录，必须读取下一个数据页，这个操作会稍微复杂一些

1. 对于普通索引来说，查找到满足条件的第一个记录后，需要查找下一个记录，直到碰到第一个不满足条件的记录
2. 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继 续检索

#### 更新过程

当需要更新一个数据页时，如果数据页在内存中就直接更新，而如果这个数据页还没有在内存中 的话，在不影响数据一致性的前提下，InooDB会将这些更新操作缓存在change buffer中，这样 就不需要从磁盘中读入这个数据页了。在下次查询需要访问这个数据页的时候，将数据页读入内 存，然后执行change buffer中与这个页有关的操作。通过这种方式就能保证这个数据逻辑的正确性，虽然名字叫作change buffer，实际上它是可以持久化的数据。也就是说，change buffer在内存中有拷贝，也会被写入到磁盘上，将change buffer中的操作应用到原数据页，得到最新结果的过程称为merge。除了访问这个数据 页会触发merge外，系统有后台线程会定期merge。在数据库正常关闭(shutdown)的过程中，也会执行merge操作，如果能够将更新操作先记录在change buffer，减少读磁盘，语句的执行速度会得到明显 的提升。而且，数据读入内存是需要占用buffer pool的，所以这种方式还能够避免占用内存，提高内存利用率

1. 对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反唯一性约束，而这必须要将数据页读入内存才能判断。如果都已经读入到内存了，那直接更新内存会更快，就没必要使用change buffer 了，因此唯一索引的更新就不能使用change buffer，实际上也只有普通索引可以使用。
2. 普通索引和唯一索引对更新语句性能影响的差别，只是一个判断，只会耗费微小的CPU时间
3. redo log 主要节省的是随 机写磁盘的IO消耗(转成顺序写)，而change buffer主要节省的则是随机读磁盘的IO消耗

#### 索引选择

普通索引和唯一索引应该怎么选择。其实，这两类索引在查询能力上 是没差别的，主要考虑的是对更新性能的影响。所以，建议尽量选择普通索引，如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭change buffer。而在其他情况下，change buffer都能提升更新性能，普通索引和change buffer的配合使用，对于数据量大的表的更新优化还是很明显的

#### 选错索引

MySQL会根据词法解析的结果分析出可能可以使用的索引作为候选项，然后在候选列表中依次判断每个索引需要扫描多少行，MySQL在真正开始执行语句之前，并不能精确地知道满足这个条件的记录有多少条，而只能根据统计信息来估算记录数，这个统计信息就是索引的“区分度”。显然，一个索引上不同的值越多，这个索引的区分度就越好，而一个索引上不同的值的个数，我们称之为“基数”，mysql采用采样的方法得到基数，因为把整张表取出来一行行统计，虽然可以得到精确的结果，但是代价太高了，所以只能选择“采样统计”，采样统计的时候，InnoDB默认会选择N个数据页，统计这些页面上的不同值，得到一个平均值，然后乘以这个索引的页面数，就得到了这个索引的基数。对于由于索引统计信息不准确导致的问题，你可以用analyze table来解决，而对于其他优化器误判的情况，你可以在应用端用force index来强行指定索引，也可以通过修改语句来引导优化器，还可以通过增加或者删除索引来绕过这个问题，主键是直接按照表的行数来估计的。而表的行数，优化器直接用的是 show table status的值

#### 前缀索引

使用前缀索引，定义好长度，就可以做到既节省空间，又不用额外增加太多的查询成本，实际上，我们在建立索引时关注的是区分度，区分度越高越好。因为区分度越高，意味着重复的键值越少。因此，我们可以通过统计索引上有多少个不同的值来判断要使用多长的前缀，使用前缀索引就用不上覆盖索引对查询性能的优化了

#### 字符串索引

1. 直接创建完整索引，这样可能比较占用空间
2. 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引
3. 倒序存储，再创建前缀索引，用于绕过字符串本身前缀的区分度不够的问题
4. 创建hash字段索引，查询性能稳定，有额外的存储和计算消耗，跟第三种方式一样，都不支 持范围扫描

#### 不走索引

1. 对索引字段做函数操作，可能会破坏索引值的有序性，因此优化器就决定放弃对走树搜索功能，优化器并不是要放弃使用这个索引，而是扫描整个索引树
2. 隐式类型转换，mylsq会将字符串转为数字进行比较，会对索引字段做函数操作，优化器会放弃走树搜索功能
3. 隐式字符编码转换，先把utf8字符串转成utf8mb4字符集，再做比较，字符集不同只是条件之一，连接过程中要求在被驱动表的索引字段 连 上加函数操作 上 ，是直接导致对被驱动表做全表扫描的原因

### 锁

#### 全局锁

就是对整个数据库实例加锁。MySQL提供了一个加全局读锁的方法，命令是 Flush tables with read lock (FTWRL)，使用场景是，做全库逻辑备份

#### 表级锁

##### 表锁

lock tables …read/write

##### 元数据锁（meta data lock，MDL)

MDL不需要显式使用，在访问一个表的时候会被自动加上。MDL的作用是，保证读写的正确性，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁，加字段(在alter table语句里面 设定等待时间，如果在这个指定的等待时间里面能够拿到MDL写锁最好，拿不到也不要阻塞后 面的业务语句，先放弃。之后开发人员或者DBA再通过重试命令重复这个过程)

#### 行锁

在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议，如果你的事务中需要锁多个行，要把最可能造成锁冲突、最可能影响并发度的锁尽量往后放

### 死锁

当并发系统中不同线程出现循环资源依赖，涉及的线程都在等待别的线程释放资源时，就会导致 这几个线程都进入无限等待的状态，称为死锁

#### 两种策略

1. 一种策略是，直接进入等待，直到超时，超时时间设置不能太短，太短的话，会出现很多误伤
2. 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行

#### 死锁检测

每个新来的被堵住的线程，都要判断会不会由于自己的加入导致了死锁，这是一个时间复杂度是O(n)的操作，死锁检测要耗费大量的CPU资源

1. 如果你能确保这个业务一定不会出现死锁，可以临时把死锁检测关掉
2. 另一个思路是控制并发度，比如同一行同时最多只有10个线程在更新，那么死锁检测的成本很低
3. 通过将一行改成逻辑上的多行来减少锁冲突

#### 解决办法

1. 检测到死锁的循环依赖，立即返回一个错误
2. 当查询的时间达到锁等待超时的设定后放弃锁请求
3. Innodb是将持有最少行级排他锁的事务进行回滚

### 事务

begin/start transaction 命令并不是一个事务的起点，在执行到它们之后的第一个操作InnoDB表 的语句，事务才真正启动。如果你想要马上启动一个事务，可以使用start transaction with consistent snapshot 这个命令

#### READ VIEW

InnoDB在实现MVCC时用到的一致性读视图，即consistent read view，用于支持

RC(Read Committed，读提交)和RR(Repeatable Read，可重复读)隔离级别的实现。

它没有物理结构，作用是事务执行期间用来定义“我能看到什么数据”，在实现上， InnoDB为每个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在“活 跃”的所有事务ID。“活跃”指的就是，启动了但还没提交，数组里面事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值加1记为高水位，这个视图数组和高水位，就组成了当前事务的一致性视图(read-view)，而数据版本的可见性规则，就是基于数据的row trx_id和这个一致性视图的对比结果得到的

#### MVCC

InnoDB里面每个事务有一个唯一的事务ID，叫作transaction id。它是在事务开始的时候向InnoDB的事务系统申请的，是按申请顺序严格递增的，而每行数据也都是有多个版本的。每次事务更新数据的时候，都会生成一个新的数据版本，并且 把transaction id赋值给这个数据版本的事务ID，记为row trx_id。同时，旧的数据版本要保留， 并且在新的数据版本中，能够有信息可以直接拿到它，也就是说，数据表中的一行记录，其实可能有多个版本(row)，每个版本有自己的row trx_id

对于当前事务的启动瞬间来说，一个数据版本的row trx_id，有以下几种可能

1. 小于等于低水位，表示这个版本是已提交的事务或者是当前事务自己生成的，这个数据是可见的
2. 大于等于高水位，表示这个版本是由将来启动的事务生成的，是肯定不可见的
3. 大于低水位，小于高水位，
   1. 若 row trx_id在数组中，表示这个版本是由还没提交的事务生成的，不可见
   2. 若 row trx_id不在数组中，表示这个版本是已经提交了的事务生成的，可见

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以 外，有三种情况:

1. 版本未提交，不可见
2. 版本已提交，但是是在视图创建后提交的，不可见
3. 版本已提交，而且是在视图创建前提交的，可见

##### 当前读

更新数据都是先读后写的，而这个读，只能读当前的 值，称为“当前读”(current read)，否则可能造成其他事务的丢失更新，除了update语句外，select语句如果加锁，也是当前读

### 可重复读

按照可重复读的定义，一个事务启动的时候，能够看到所有已经提交的事务结果。但是之后，这 个事务执行期间，其他事务的更新对它不可见

#### 实现

可重复读的核心就是一致性读(consistent read);而事务更新数据的时候，只能用当前读。如果当前的记录的行锁被其他事务占用的话，就需要进入锁等待

#### 与读提交区别

1. 在可重复读隔离级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询 都共用这个一致性视图

2. 在读提交隔离级别下，每一个语句执行前都会重新算出一个新的视图

   为什么表结构不支持“可重复读”?这是因为表结构没有对应的行数据，也没有 row trx_id，因此只能遵循当前读的逻辑

### 怎么删除表的前10000行

在一个连接中循环执行20次 delete from T limit 500

### 事务日志

事务日志可以帮助提高事务的效率，使用事务日志，存储引擎在修改表的数据时只需要修改其内存拷贝，再把该修改行为记录持久到硬盘上的事务日志中，而不用每次都将修改的数据本身持久到磁盘，事务日志采用的是追加的方式，，因此写日志的操作是磁盘上一小块区域的顺序io，事务日志持久化后，内存中被修改的数据在后台可以慢慢地刷回到磁盘，通常称为预写日志，修改数据需要写两次磁盘，如果数据的修改已经记录到事务日志并持久化，但数据本身还没有写回磁盘，此时系统崩溃，存储引擎在重启时能够自动恢复这部分修改的数据

### 两阶段锁定协议

在事务执行过程中，随时都可以执行锁定，但锁只有在执行commit或者rollback时才会释放，并且所有的锁是在同一时刻被释放

### MVCC

MVCC的实现，是通过保存数据在某个时间点的快照来实现的，不管需要执行多长时间，每个事务看到的数据都是一致的，根据事务的开始时间不同，每个事务对同一张表，同一时刻看到的数据可能是不一样的，MVCC只在READ COMMITTED和REPEATABLE READ两个隔离级别下工作

#### Inonodb的实现

Innodb的MVCC，是通过在每行记录后面保存两个隐藏的列来实现的，这两个列，一个保存了行的创建版本号，一个保存行的删除版本号，每开始一个新的事务，系统版本号都会自动递增，事务开始时刻的版本号会作为事务的版本号，insert：innodb为新插入的每一行保存当前系统版本号作为行版本号，delete：innodb为删除的每一行保存当前系统版本号作为行删除标识，update：innodb为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识

#### RR级别MVCC

Innodb会根据以下两个条件检查每行记录

1. Innodb只查找版本早于当前事务版本的数据行(也就是说，行的系统版本号小于等于事务的系统版本号)，这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的
2. 行的删除版本要么未定义，要么大于当前事务版本号，这样可以确保事务读取到的行，在事务开始之前未被删除

### Innodb底层机制

#### 刷盘

当内存数据页跟磁盘数据页内容不一致的时候，我们称这个内存页为“脏页”。内存数据写入到磁盘后，内存和磁盘上的数据页的内容就一致了，称为“干净页”，InnoDB用缓冲池 (buffer pool)管理内存，buffer pool位于存储引擎层，查询缓存位于server层

1. 当InnoDB的redo log写满了，这时候系统会停止所有更新操作，把 checkpoint往前推进，redo log留出空间可以继续写，把checkpoint位置从CP推进到CP’，就需要将两个点之间的日志对应的所有脏页都flush到磁盘上
2. 当系统内存不足。当需要新的内存页，而内存不够用的时候，就要淘汰一些数据页，空出内存给别的数据页使用。如果淘汰的是“脏页”，就要先将脏页写到磁盘
3. 当MySQL认为系统“空闲”的时候
4. 当MySQL正常关闭的情况。这时候，MySQL会把内存的脏页都flush到磁 盘上，这样下次MySQL启动的时候，就可以直接从磁盘上读数据，启动速度会很快

##### 对性能的影响

InnoDB的策略是尽量使用内存，因此对于一个长时间运行的库来说，未被使用的页面很少。而当要读入的数据页没有在内存的时候，就必须到缓冲池中申请一个数据页。这时候只能把最久 不使用的数据页从内存中淘汰掉:如果要淘汰的是一个干净页，就直接释放出来复用;但如果是脏页呢，就必须将脏页先刷到磁盘，变成干净页后才能复用，无论是你的查询语句在需要内存的时候可能要求淘汰一个脏页，还是由于刷脏页的逻辑会占用IO资源 并可能影响到了你的更新语句，都可能是造成你从业务端感知到MySQL“抖”了一下的原因，一旦一个查询请求需要在执行过程中先flush掉一个脏页时，这个查询就可能要比平时慢了。而 MySQL中的一个机制，可能让你的查询会更慢:在准备刷一个脏页的时候，如果这个数据页旁 边的数据页刚好是脏页，就会把这个“邻居”也带着一起刷掉;而且这个把“邻居”拖下水的逻辑还 可以继续蔓延，这个优化在机械硬盘时代是很有意义的，可以减少很多随机IO

#### InnoDB刷脏页的控制策略

当前的脏页比例，redo log写入速度

#### 参数innodb_file_per_table

表数据既可以存在共享表空间里，也可以是单独的文件，建议将这个值设置为ON。因为，一个表单独存储为一个文件更容易管理，而且在你不需要这个表的时候，通过drop table命令，系统就会直接删除这个文 件。而如果是放在共享表空间中，即使表删掉了，空间也是不会回收的

### select count(*)慢

#### 实现方式

1. MyISAM引擎把一个表的总行数存在了磁盘上，因此执行count(*)的时候会直接返回这个数，效率很高，如果加了where 条件 

   的话，MyISAM表也是不能返回得这么快的

2. 而InnoDB引擎就麻烦了，它执行count()的时候，需要把数据一行一行地从引擎里面读出来，然后累积计数，InnoDB是索引组织表，主键索引树的叶子节点是数据，而普通索引树的叶子节点是 主键值。所以，普通索引树比主键索引树小很多。对于count()这样的操作，遍历哪个索引树得 到的结果逻辑上都是一样的。因此，MySQL优化器会找到最小的那棵树来遍历。在保证逻辑在确的前提下，尽量减少扫描的数据量，是数据库系统设计的通用法则之一。

#### 解决办法

1. 用缓存系统保存计数，有丢失数据和计数不精确的问题，把计数放在Redis里面，不能够保证计数和MySQL表里的数据精确一致的原因，是这两个这 不同的存储构成的系统，不支持分布式事务，无法拿到精确一致的视图
2. 在数据库保存计数，而把计数值也放在MySQL中，就解决了一致性视图的问题

#### 不同count区别

count()是一个聚合函数，对于返回的结果集，一行行地 判断，如果count函数的参数不是NULL，累计值就加1，否则不加。最后返回累计值，按照效率排序的话，count(字段)<count(主键id)<count(1)≈count(*)，所以建议尽量使用count(*)

1. 对 count( 主键id) 说来 ，InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server 层。server层拿到id后，判断是不可能为空的，就按行累加
2. 对 count(1) 来说 ，InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个 数字“1”进去，判断是不可能为空的，按行累加
3. 对 count(字段)来说，如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加，如果这个“字段”定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加
4. 但 count(星)是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(星)肯定不是null，按行累加

### order by

sort_buffer_size，就是MySQL为排序开辟的内存（sort_buffer）的大小。如果要排序的数据量 小于sort_buffer_size，排序就在内存中完成。但如果排序数据量太大，内存放不下，则不得不利用磁盘临时文件辅助排序，外部排序一般使用归并排序算法。可以这么简单理 解，MySQL 将需要排序的数据分成 将 12份，每一份单独排序后存在这些临时文件中。然后把 份 这12个有序文件再合并成一个有序的大文件

#### 全字段排序 全 VS rowid V 排序

如果MySQL实在是担心排序内存太小，会影响排序效率，才会采用rowid排序算法，这样排序过程中一次可以排序更多行，但是需要再回到原表去取数据，如果MySQL认为内存足够大，会优先选择全字段排序，把需要的字段都放到sort_buffer中，这样排序后就会直接从内存里面返回查询结果了，不用再回到原表去取数据

